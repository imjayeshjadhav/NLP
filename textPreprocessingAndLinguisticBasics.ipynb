{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10df22dc",
   "metadata": {},
   "source": [
    "## Text Preprocessing and Linguistic Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee0c7cc",
   "metadata": {},
   "source": [
    "#### Aim: TO understand how raw text is converted into clean, structured data using NLP preprocessing techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51aa367e",
   "metadata": {},
   "source": [
    "#### Techniques\n",
    "- Tokenization\n",
    "- Stopword Removal\n",
    "- Punctuation Removal\n",
    "- Stemming\n",
    "- Lemmatization\n",
    "- Morphology \n",
    "- Frequency Analysis \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8251697c",
   "metadata": {},
   "source": [
    "#### Theory :\n",
    "Raw text is noisy, Machines cannot directly understand: punctuation, different forms, unnecessary common words. Preprocessing makes text suitable for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19c4d7e",
   "metadata": {},
   "source": [
    "#### Step 1: Input Text (Raw Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "574ee825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Text:\n",
      " \n",
      "In 2026, text preprocessing has evolved to prioritize context-aware noise reduction, ensuring that LLMs interpret subtle linguistic nuances accurately. Modern practitioners now integrate ethical debiasing directly into the cleaning phase to mitigate algorithmic prejudice before model training begins. By mastering these refined cleaning pipelines, students bridge the gap between raw, messy human dialogue and sophisticated machine intelligence.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text= \"\"\"\n",
    "In 2026, text preprocessing has evolved to prioritize context-aware noise reduction, ensuring that LLMs interpret subtle linguistic nuances accurately. Modern practitioners now integrate ethical debiasing directly into the cleaning phase to mitigate algorithmic prejudice before model training begins. By mastering these refined cleaning pipelines, students bridge the gap between raw, messy human dialogue and sophisticated machine intelligence.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Raw Text:\\n\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c35f1c",
   "metadata": {},
   "source": [
    "#### Step 2: Text Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80f6aa7",
   "metadata": {},
   "source": [
    "##### Cleaning means removing unnecessary symbols and normalizing text, \n",
    "Why?\n",
    "1. \"Language\" and \"language\" should be treated same.\n",
    "2. Punctuation like .,!? has no meaning in most NLP tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "007905bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Text:\n",
      " \n",
      "in 2026text preprocessing has evolved to prioritize context-aware noise reductionensuring that llms interpret subtle linguistic nuances accuratelymodern practitioners now integrate ethical debiasing directly into the cleaning phase to mitigate algorithmic prejudice before model training beginsby mastering these refined cleaning pipelinesstudents bridge the gap between rawmessy human dialogue and sophisticated machine intelligence.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re # Regular expression library used to search and modify text\n",
    "\n",
    "# Remove everything except letter and spaces \n",
    "# Internally \n",
    "# [^a-zA-Z] means: anything NOT a letter or space \n",
    "# re.sub replaces sub characters with empty strings \n",
    "\n",
    "clean_text = re.sub(r'[^a-zA-Z] ','',text)\n",
    "# Parametres of re (pattern, replace, string, count=0, flags=0)\n",
    "\n",
    "# Convert all text to lowercase \n",
    "# This avoids treating \"NLP\" and \"nlp\" as different words \n",
    "\n",
    "clean_text = clean_text.lower()\n",
    "\n",
    "print(\"Cleaned Text:\\n\", clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1332b2a",
   "metadata": {},
   "source": [
    "#### Step 3: Tokenization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243502b2",
   "metadata": {},
   "source": [
    "##### Breaking text into smaller pieces called tokens \n",
    "Tokens can be: Sentences, Words\n",
    "\n",
    "NLP models do not work on paragraphs \n",
    "\n",
    "They work on tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f27aed53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence Tokens:\n",
      "- \n",
      "In 2026, text preprocessing has evolved to prioritize context-aware noise reduction, ensuring that LLMs interpret subtle linguistic nuances accurately.\n",
      "- Modern practitioners now integrate ethical debiasing directly into the cleaning phase to mitigate algorithmic prejudice before model training begins.\n",
      "- By mastering these refined cleaning pipelines, students bridge the gap between raw, messy human dialogue and sophisticated machine intelligence.\n",
      "\n",
      "Word Tokens:\n",
      "['in', '2026text', 'preprocessing', 'has', 'evolved', 'to', 'prioritize', 'context-aware', 'noise', 'reductionensuring', 'that', 'llms', 'interpret', 'subtle', 'linguistic', 'nuances', 'accuratelymodern', 'practitioners', 'now', 'integrate', 'ethical', 'debiasing', 'directly', 'into', 'the', 'cleaning', 'phase', 'to', 'mitigate', 'algorithmic', 'prejudice', 'before', 'model', 'training', 'beginsby', 'mastering', 'these', 'refined', 'cleaning', 'pipelinesstudents', 'bridge', 'the', 'gap', 'between', 'rawmessy', 'human', 'dialogue', 'and', 'sophisticated', 'machine', 'intelligence', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('punkt_tab') # To download recent updates\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# Sentence Tokenization -> Splits paragraph into sentences using punctuation rules\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "words = word_tokenize(clean_text)\n",
    "\n",
    "print(\"\\nSentence Tokens:\")\n",
    "for s in sentences:\n",
    "    print(\"-\",s)\n",
    "\n",
    "print(\"\\nWord Tokens:\")\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ccd738",
   "metadata": {},
   "source": [
    "#### Step 4: Stopword Removal\n",
    "Stopwords are very common words like: is, am, are, the, in, and, to, of.\n",
    "\n",
    "These words appear frequently but do not add meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dbbdd1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After Stopword Removal\n",
      "['2026text', 'preprocessing', 'evolved', 'prioritize', 'context-aware', 'noise', 'reductionensuring', 'llms', 'interpret', 'subtle', 'linguistic', 'nuances', 'accuratelymodern', 'practitioners', 'integrate', 'ethical', 'debiasing', 'directly', 'cleaning', 'phase', 'mitigate', 'algorithmic', 'prejudice', 'model', 'training', 'beginsby', 'mastering', 'refined', 'cleaning', 'pipelinesstudents', 'bridge', 'gap', 'rawmessy', 'human', 'dialogue', 'sophisticated', 'machine', 'intelligence', '.']\n"
     ]
    }
   ],
   "source": [
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "# Remove stopwords from tokens \n",
    "# Internally:\n",
    "# For each word, check if it exists in stop_words set\n",
    "# If not, keep it \n",
    "\n",
    "filteredWords =[] # List\n",
    "for w in words:\n",
    "    if w.lower() not in stopWords:\n",
    "        filteredWords.append(w)\n",
    "\n",
    "print(\"\\nAfter Stopword Removal\")\n",
    "print(filteredWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c43fba",
   "metadata": {},
   "source": [
    "#### Step 5: Stemming\n",
    "Cutting word endings to get root form.\n",
    "\n",
    "It uses simple rules, not dictionary.\n",
    "\n",
    "Examples:\n",
    "1. playing -> play\n",
    "2. studies -> studi (not correct English, but acceptable for machines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cb6380ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stemmed Words:\n",
      "['2026text', 'preprocess', 'evolv', 'priorit', 'context-awar', 'nois', 'reductionensur', 'llm', 'interpret', 'subtl', 'linguist', 'nuanc', 'accuratelymodern', 'practition', 'integr', 'ethic', 'debias', 'directli', 'clean', 'phase', 'mitig', 'algorithm', 'prejudic', 'model', 'train', 'beginsbi', 'master', 'refin', 'clean', 'pipelinesstud', 'bridg', 'gap', 'rawmessi', 'human', 'dialogu', 'sophist', 'machin', 'intellig', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "stemmed_words = []\n",
    "for w in filteredWords:\n",
    "    root = stemmer.stem(w)   # Removes suffix based on algorithm rules\n",
    "    stemmed_words.append(root)\n",
    "\n",
    "print(\"\\nStemmed Words:\")\n",
    "print(stemmed_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81113ff",
   "metadata": {},
   "source": [
    "#### Step 6: Lemmatization\n",
    "Converting word to dictionary form (lemma)\n",
    "\n",
    "It considers grammer and meaning \n",
    "\n",
    "Examples:\n",
    "1. playing -> play\n",
    "2. better -> good\n",
    "\n",
    "More accurate but slower than stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f7ba0a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lemmatized Words:\n",
      "['2026text', 'preprocessing', 'evolved', 'prioritize', 'context-aware', 'noise', 'reductionensuring', 'llm', 'interpret', 'subtle', 'linguistic', 'nuance', 'accuratelymodern', 'practitioner', 'integrate', 'ethical', 'debiasing', 'directly', 'cleaning', 'phase', 'mitigate', 'algorithmic', 'prejudice', 'model', 'training', 'beginsby', 'mastering', 'refined', 'cleaning', 'pipelinesstudents', 'bridge', 'gap', 'rawmessy', 'human', 'dialogue', 'sophisticated', 'machine', 'intelligence', '.']\n"
     ]
    }
   ],
   "source": [
    "# nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmatizedWords =[]\n",
    "\n",
    "for w in filteredWords:\n",
    "    lemma = lemmatizer.lemmatize(w)\n",
    "    lemmatizedWords.append(lemma)\n",
    "\n",
    "print(\"\\nLemmatized Words:\")\n",
    "print(lemmatizedWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f614aac2",
   "metadata": {},
   "source": [
    "#### Step 7: Morphology\n",
    "Study of word structure \n",
    "\n",
    "Word is made of: prefix + root + suffix\n",
    "\n",
    "Example: replaying -> re + play + ing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2f679254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Morphological Analysis\n",
      "{'word': '2026text', 'prefix': '20', 'root': '2026text', 'suffix': 'ext'}\n",
      "{'word': 'preprocessing', 'prefix': 'pr', 'root': 'preprocessing', 'suffix': 'ing'}\n",
      "{'word': 'evolved', 'prefix': 'ev', 'root': 'evolved', 'suffix': 'ved'}\n",
      "{'word': 'prioritize', 'prefix': 'pr', 'root': 'prioritize', 'suffix': 'ize'}\n",
      "{'word': 'context-aware', 'prefix': 'co', 'root': 'context-aware', 'suffix': 'are'}\n",
      "{'word': 'noise', 'prefix': 'no', 'root': 'noise', 'suffix': 'ise'}\n",
      "{'word': 'reductionensuring', 'prefix': 're', 'root': 'reductionensuring', 'suffix': 'ing'}\n",
      "{'word': 'llms', 'prefix': 'll', 'root': 'llms', 'suffix': 'lms'}\n",
      "{'word': 'interpret', 'prefix': 'in', 'root': 'interpret', 'suffix': 'ret'}\n",
      "{'word': 'subtle', 'prefix': 'su', 'root': 'subtle', 'suffix': 'tle'}\n",
      "{'word': 'linguistic', 'prefix': 'li', 'root': 'linguistic', 'suffix': 'tic'}\n",
      "{'word': 'nuances', 'prefix': 'nu', 'root': 'nuances', 'suffix': 'ces'}\n",
      "{'word': 'accuratelymodern', 'prefix': 'ac', 'root': 'accuratelymodern', 'suffix': 'ern'}\n",
      "{'word': 'practitioners', 'prefix': 'pr', 'root': 'practitioners', 'suffix': 'ers'}\n",
      "{'word': 'integrate', 'prefix': 'in', 'root': 'integrate', 'suffix': 'ate'}\n",
      "{'word': 'ethical', 'prefix': 'et', 'root': 'ethical', 'suffix': 'cal'}\n",
      "{'word': 'debiasing', 'prefix': 'de', 'root': 'debiasing', 'suffix': 'ing'}\n",
      "{'word': 'directly', 'prefix': 'di', 'root': 'directly', 'suffix': 'tly'}\n",
      "{'word': 'cleaning', 'prefix': 'cl', 'root': 'cleaning', 'suffix': 'ing'}\n",
      "{'word': 'phase', 'prefix': 'ph', 'root': 'phase', 'suffix': 'ase'}\n",
      "{'word': 'mitigate', 'prefix': 'mi', 'root': 'mitigate', 'suffix': 'ate'}\n",
      "{'word': 'algorithmic', 'prefix': 'al', 'root': 'algorithmic', 'suffix': 'mic'}\n",
      "{'word': 'prejudice', 'prefix': 'pr', 'root': 'prejudice', 'suffix': 'ice'}\n",
      "{'word': 'model', 'prefix': 'mo', 'root': 'model', 'suffix': 'del'}\n",
      "{'word': 'training', 'prefix': 'tr', 'root': 'training', 'suffix': 'ing'}\n",
      "{'word': 'beginsby', 'prefix': 'be', 'root': 'beginsby', 'suffix': 'sby'}\n",
      "{'word': 'mastering', 'prefix': 'ma', 'root': 'mastering', 'suffix': 'ing'}\n",
      "{'word': 'refined', 'prefix': 're', 'root': 'refined', 'suffix': 'ned'}\n",
      "{'word': 'cleaning', 'prefix': 'cl', 'root': 'cleaning', 'suffix': 'ing'}\n",
      "{'word': 'pipelinesstudents', 'prefix': 'pi', 'root': 'pipelinesstudents', 'suffix': 'nts'}\n",
      "{'word': 'bridge', 'prefix': 'br', 'root': 'bridge', 'suffix': 'dge'}\n",
      "{'word': 'gap', 'prefix': 'ga', 'root': 'gap', 'suffix': 'gap'}\n",
      "{'word': 'rawmessy', 'prefix': 'ra', 'root': 'rawmessy', 'suffix': 'ssy'}\n",
      "{'word': 'human', 'prefix': 'hu', 'root': 'human', 'suffix': 'man'}\n",
      "{'word': 'dialogue', 'prefix': 'di', 'root': 'dialogue', 'suffix': 'gue'}\n",
      "{'word': 'sophisticated', 'prefix': 'so', 'root': 'sophisticated', 'suffix': 'ted'}\n",
      "{'word': 'machine', 'prefix': 'ma', 'root': 'machine', 'suffix': 'ine'}\n",
      "{'word': 'intelligence', 'prefix': 'in', 'root': 'intelligence', 'suffix': 'nce'}\n",
      "{'word': '.', 'prefix': '.', 'root': '.', 'suffix': '.'}\n"
     ]
    }
   ],
   "source": [
    "def morphologicalAnalysis(word):\n",
    "    # Simple demonstration \n",
    "    return {\n",
    "        \"word\":word,\n",
    "        \"prefix\": word[:2], # first 2 letters\n",
    "        \"root\":word, # In real NLP, root is found by stemmer/lemmatizer\n",
    "        \"suffix\":word[-3:] # Last 3 letters \n",
    "    }\n",
    "\n",
    "print(\"Morphological Analysis\")\n",
    "for w in filteredWords:\n",
    "    print(morphologicalAnalysis(w))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca62788",
   "metadata": {},
   "source": [
    "#### Step 8: Frequency Analysis\n",
    "Counting how often words appear\n",
    "\n",
    "Used in:\n",
    "1. Search engines \n",
    "2. TF-IDF\n",
    "3. Topic detection \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a80e56e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word Frequency\n",
      "2026text :  1\n",
      "preprocessing :  1\n",
      "evolved :  1\n",
      "prioritize :  1\n",
      "noise :  1\n",
      "reductionensuring :  1\n",
      "llm :  1\n",
      "interpret :  1\n",
      "subtle :  1\n",
      "linguistic :  1\n",
      "nuance :  1\n",
      "accuratelymodern :  1\n",
      "practitioner :  1\n",
      "integrate :  1\n",
      "ethical :  1\n",
      "debiasing :  1\n",
      "directly :  1\n",
      "cleaning :  2\n",
      "phase :  1\n",
      "mitigate :  1\n",
      "algorithmic :  1\n",
      "prejudice :  1\n",
      "model :  1\n",
      "training :  1\n",
      "beginsby :  1\n",
      "mastering :  1\n",
      "refined :  1\n",
      "pipelinesstudents :  1\n",
      "bridge :  1\n",
      "gap :  1\n",
      "rawmessy :  1\n",
      "human :  1\n",
      "dialogue :  1\n",
      "sophisticated :  1\n",
      "machine :  1\n",
      "intelligence :  1\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter \n",
    "frequency = Counter(\n",
    "    [\n",
    "        w for w in lemmatizedWords if w.isalnum()\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"\\nWord Frequency\")\n",
    "\n",
    "for word, count in frequency.items():\n",
    "    print(word,\": \", count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
